{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios 5-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (Entrega obligatoria individual en repo) Genere un video en un patio o en un hall de edificio donde en un principio se vea vacío y luego aparezca una persona. Mediante los métodos de motion detection (sin usar deep learning) logre una detección de la persona cuando entra al cuadro suponiendo la utilidad para una cámara de seguridad. \n",
    "Luego sobre el mismo video aplique los algoritmos de flujo denso y disperso que se mostraron en clase. \n",
    "Escriba una reflexión sobre los resultados en el formato md dentro del Jupyter Notebook.\n",
    "6. (Entrega obligatoria individual en repo) Explique cuál es diferencia entre localización de objetos y clasificación de imágenes. Muestre ejemplos de ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "Sin morfologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin morfologia\n",
    "def process_frame_difference(new_image, prev_image, **kwargs):\n",
    "    \n",
    "    new_gray = cv.cvtColor(new_image, cv.COLOR_RGB2GRAY) # Convertir las imágenes a escala de grises\n",
    "    prev_gray = cv.cvtColor(prev_image, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    frame_diff = cv.absdiff(new_gray, prev_gray) # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
    "\n",
    "    norm_diff = cv.normalize(frame_diff, None, 0, 255, cv.NORM_MINMAX) # Normalizar la imagen de diferencia\n",
    "\n",
    "    _, thresh = cv.threshold(norm_diff, 150, 255, cv.THRESH_BINARY) # Umbralizar la imagen para resaltar las diferencias\n",
    "\n",
    "    thresh_color = cv.cvtColor(thresh, cv.COLOR_GRAY2RGB)  # Convertir la imagen umbralizada a color para mantener la consistencia con el video original\n",
    "\n",
    "    return thresh_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"camara.mp4\"\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "ret, prev_frame = cap.read()\n",
    "prev_frame = cv.cvtColor(prev_frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Procesar el fotograma actual utilizando la función process_frame_difference\n",
    "    processed_frame = process_frame_difference(frame_rgb, prev_frame)\n",
    "\n",
    "    cv.imshow('Procesamiento de Frame Difference', processed_frame)\n",
    "    cv.imshow('Video original', frame_rgb)\n",
    "\n",
    "    prev_frame = frame_rgb\n",
    "\n",
    "    if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Utilizadn odilatacion y contornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_contours(image, contours):\n",
    "    \"\"\"Dibuja los contornos sobre la imagen.\"\"\"\n",
    "    result_image = image.copy()\n",
    "    cv.drawContours(result_image, contours, -1, (0, 255, 0), 2)\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_difference_full(new_image, prev_image, **kwargs):\n",
    "    new_gray = cv.cvtColor(new_image, cv.COLOR_RGB2GRAY) # Convertir las imágenes a escala de grises\n",
    "    prev_gray = cv.cvtColor(prev_image, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    frame_diff = cv.absdiff(new_gray, prev_gray) # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
    "\n",
    "    norm_diff = cv.normalize(frame_diff, None, 0, 255, cv.NORM_MINMAX)    # Normalizar la imagen de diferencia\n",
    "\n",
    "    _, thresh = cv.threshold(norm_diff, 150, 255, cv.THRESH_BINARY)     # Umbralizar la imagen para resaltar las diferencias\n",
    "\n",
    "    kernel = np.ones((5,5),np.uint8)  # Dilatar la imagen umbralizada para mejorar la detección de contornos\n",
    "    dilated = cv.dilate(thresh, kernel, iterations = 1)\n",
    "\n",
    "    dilated = dilated.astype(np.uint8) # Convertir la imagen dilatada a formato adecuado para findContours\n",
    "\n",
    "    contours, _ = cv.findContours(dilated, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)     # Encontrar contornos en la imagen dilatada\n",
    "\n",
    " \n",
    "    if kwargs.get('draw_mode', 0) == 0:    # Dibujar cuadros delimitadores alrededor de los contornos\n",
    "      result_image = draw_contours(new_image, contours)\n",
    "    elif kwargs.get('draw_mode', 0) == 1:\n",
    "      result_image = draw_contours(thresh, contours)\n",
    "\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"camara.mp4\"\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "ret, prev_frame = cap.read()\n",
    "prev_frame = cv.cvtColor(prev_frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    processed_frame = process_frame_difference_full(frame_rgb, prev_frame, draw_mode=0)  # Procesar el fotograma actual utilizando la función process_frame_difference_full\n",
    "\n",
    "    cv.imshow('Procesamiento de Frame Difference', processed_frame)\n",
    "    cv.imshow('Video original', frame_rgb)\n",
    "\n",
    "    prev_frame = frame_rgb\n",
    "\n",
    "    if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "Flujo optico disperso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sparse_optical_flow(new_image, prev_image):\n",
    "    \n",
    "    new_gray = cv.cvtColor(new_image, cv.COLOR_BGR2GRAY) # Preparamos las imagenes de trabajo\n",
    "    prev_gray_image = cv.cvtColor(prev_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Verificar si ya se han detectado las características de Shi-Tomasi\n",
    "    if not hasattr(process_sparse_optical_flow, \"shi_tomasi_done\"):\n",
    "        # Definir parámetros para la detección de esquinas de Shi-Tomasi\n",
    "        feature_params = dict(maxCorners=200, qualityLevel=0.05, minDistance=20, blockSize=7) # Puedo  ajustar estos valores para detectar solo la persona Reducir, Redcuir, aumentar.\n",
    "        # Detectar puntos característicos en la imagen\n",
    "        process_sparse_optical_flow.prev_points = cv.goodFeaturesToTrack(new_gray, mask=None, **feature_params)\n",
    "        # Crear una máscara para dibujar el flujo óptico\n",
    "        process_sparse_optical_flow.mask = np.zeros_like(new_image)\n",
    "        # Marcar que se ha completado la detección de Shi-Tomasi\n",
    "        process_sparse_optical_flow.shi_tomasi_done = True\n",
    "\n",
    "    # Continuar si se ha completado la detección de Shi-Tomasi\n",
    "    if process_sparse_optical_flow.shi_tomasi_done:\n",
    "        prev_points = process_sparse_optical_flow.prev_points\n",
    "        mask = process_sparse_optical_flow.mask\n",
    "\n",
    "    # Parámetros para el flujo óptico de Lucas-Kanade\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                     criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03)) # Achicar la ventana puede ayudar, mezclar el otro.\n",
    "\n",
    "    # Calcular el flujo óptico de Lucas-Kanade\n",
    "    new_points, status, error = cv.calcOpticalFlowPyrLK(prev_gray_image, new_gray, prev_points, None, **lk_params)\n",
    "    # Filtrar puntos buenos\n",
    "    good_old = prev_points[status == 1]\n",
    "    good_new = new_points[status == 1]\n",
    "    color = (100, 255, 0)  # Color para el dibujo\n",
    "    # Dibujar el movimiento (flujo óptico)\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.astype(int).ravel()\n",
    "        c, d = old.astype(int).ravel()\n",
    "        mask = cv.line(mask, (a, b), (c, d), color, 2)\n",
    "        new_image = cv.circle(new_image, (a, b), 3, color, -1)\n",
    "\n",
    "    # Combinar la imagen actual con las líneas de flujo óptico dibujadas\n",
    "    output = cv.add(new_image, mask)\n",
    "    # Actualizar puntos para el siguiente cuadro\n",
    "    process_sparse_optical_flow.prev_points = good_new.reshape(-1, 1, 2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de video\n",
    "video_path = \"camara.mp4\"\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "ret, prev_frame = cap.read()\n",
    "prev_frame = cv.cvtColor(prev_frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Procesar el fotograma actual utilizando la función process_sparse_optical_flow\n",
    "    processed_frame = process_sparse_optical_flow(frame_rgb, prev_frame)\n",
    "\n",
    "    cv.imshow('Procesamiento de Flujo Óptico', processed_frame)\n",
    "    cv.imshow('Video original', frame_rgb)\n",
    "\n",
    "    prev_frame = frame_rgb\n",
    "\n",
    "    if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "Flujo optico denso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar el flujo óptico denso\n",
    "def process_dense_optical_flow(new_image, prev_image):\n",
    "    # Convierte la nueva imagen a escala de grises\n",
    "    gray = cv.cvtColor(new_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    if not hasattr(process_dense_optical_flow, \"init_done\"):\n",
    "        process_dense_optical_flow.prev_gray = cv.cvtColor(new_image, cv.COLOR_BGR2GRAY)\n",
    "        process_dense_optical_flow.mask = np.zeros_like(new_image)\n",
    "        process_dense_optical_flow.mask[..., 1] = 255\n",
    "        process_dense_optical_flow.init_done = True\n",
    "\n",
    "    if process_dense_optical_flow.init_done:\n",
    "        prev_gray = process_dense_optical_flow.prev_gray\n",
    "        mask = process_dense_optical_flow.mask\n",
    "\n",
    "    # Calcula el flujo óptico\n",
    "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0) # Se puede modificar los valores\n",
    "\n",
    "    # Computa magnitud y ángulo de los vectores 2D\n",
    "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # Establece el tono de la imagen según la dirección del flujo óptico\n",
    "    mask[..., 0] = angle * 180 / np.pi / 2\n",
    "\n",
    "    # Establece el valor de la imagen según la magnitud del flujo óptico\n",
    "    mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
    "\n",
    "    # Convierte de HSV a RGB\n",
    "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Actualiza la imagen previa a gris\n",
    "    process_dense_optical_flow.prev_grayprev_gray = gray.copy()\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de video\n",
    "video_path = \"terraza.mp4\"\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "ret, prev_frame = cap.read()\n",
    "prev_frame = cv.cvtColor(prev_frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Procesar el fotograma actual utilizando la función process_sparse_optical_flow\n",
    "    processed_frame = process_dense_optical_flow(frame_rgb, prev_frame)\n",
    "\n",
    "    cv.imshow('Procesamiento de Flujo Denso', processed_frame)\n",
    "    cv.imshow('Video original', frame_rgb)\n",
    "\n",
    "    prev_frame = frame_rgb\n",
    "\n",
    "    if cv.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflexion + Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflexion de resultados.\n",
    "_Los resultados son muy dependientes de que tan fija esté la cámara, de los movimientos que existan también. Dependen demasiado de los umbrales, y al ir ajustandolos recibimos distintos resultados. Sufrí mucho ajustando unos y otros, grabando diferentes videos y fue lo mejor que conseguí. Entiendo que por su naturaleza nunca va a ser perfecto salvo que el video lo sea._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencia e/ Localizacion de objetos y Clasifiacion de Imagenes\n",
    "\n",
    "_Por lo que pude entender de la teoría, la principal diferencia la vemos en el enfoque._\n",
    "_La localización de objetos identifica y delimita un objeto en una imagen y su objetivo, entiendo, es identificar que ahí 'posicion' hay un objeto, es más importante saber dónde está el objeto en la imágen._\n",
    "_Por ejemplo, detectar los autos en el video de ejemplo_\n",
    "\n",
    "_Mientras que en la clasifiación de imágenes el enfoque cambia, refiere más a asignar una etiqueta a lo que se encuentra en la misma.El objetivo principal es más el de identificar la categoría o etiqueta a la que pertenece la imágen en conjunto. Más simple, entiendo que su principal tarea es asignar etiquetas a lo que encontramos, por ejempo, pajaro, perro, gato._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cellphone.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
